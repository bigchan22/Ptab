{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0acdc824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-18 12:04:35.327318: W external/org_tensorflow/tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64::/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/extras/CUPTI/lib64\n",
      "2023-03-18 12:04:35.328423: W external/org_tensorflow/tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64::/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/extras/CUPTI/lib64\n",
      "/root/anaconda3/envs/mpnn/lib/python3.9/site-packages/haiku/_src/data_structures.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import dataclasses\n",
    "import datetime\n",
    "import enum\n",
    "import functools\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import tempfile\n",
    "from typing import Sequence\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import optax\n",
    "import psutil\n",
    "import scipy.sparse as sp\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b9f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BH.data_loader import *\n",
    "from BH.generate_data import *\n",
    "from Model import Model,Direction,Reduction\n",
    "# from Load_Data import batch\n",
    "from Train import train,print_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59708505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input data...\n",
      "Generating data from the directory /Data/Ptab/n=5\n"
     ]
    }
   ],
   "source": [
    "#@title Load data\n",
    "\n",
    "#@markdown Training this model is pretty slow - an hour or so on the free tier colab, but subject to inactivity timeouts and pre-emptions.\n",
    "\n",
    "#@markdown In order to make it possible to recreate the results from the paper reliably and quickly, we provide several helpers to either speed things up, or reduce the memory footprint:\n",
    "#@markdown * Pretrained weights - greatly speeds things up by loading the trained model parameters rather than learning from the data\n",
    "#@markdown * If you are running on a high memory machine (ie *not* on the free colab instance!) the input graph data can be loaded from a pickle (which is faster to load) and kept in memory (faster to re-use, but uses ~12Gb of memory). This makes no difference to training speed (it's only relevant for `generate_graph_data()` and `get_saliency_vectors()`).\n",
    "DIR_PATH = \"/Data/Ptab/n=5\"\n",
    "\n",
    "\n",
    "use_pretrained_weights = True  #@param{type:\"boolean\"}\n",
    "hold_graphs_in_memory = False  #@param{type:\"boolean\"}\n",
    "\n",
    "gb = 1024**3\n",
    "total_memory = psutil.virtual_memory().total / gb\n",
    "# Less than 20Gb of RAM means we need to do some things slower, but with lower memory impact - in\n",
    "# particular, we want to allow things to run on the free colab tier.\n",
    "if total_memory < 20 and hold_graphs_in_memory:\n",
    "    raise RuntimeError(f\"It is unlikely your machine (with {total_memory}Gb) will have enough memory to complete the colab's execution!\")\n",
    "\n",
    "print(\"Loading input data...\")\n",
    "full_dataset, train_dataset, test_dataset = load_input_data(DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59ff2656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Network Setup\n",
    "\n",
    "step_size = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "num_classes = np.max(train_dataset.labels) + 1\n",
    "model = Model(\n",
    "    num_layers=3,\n",
    "    num_features=64,\n",
    "    num_classes=num_classes,\n",
    "    direction=Direction.BOTH,\n",
    "    reduction=Reduction.SUM,\n",
    "    apply_relu_activation=True,\n",
    "    use_mask=False,\n",
    "    share=False,\n",
    "    message_relu=True,\n",
    "    with_bias=True)\n",
    "\n",
    "loss_val_gr = jax.value_and_grad(model.loss)\n",
    "opt_init, opt_update = optax.adam(step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3943b6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b55c3030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/root/anaconda3/envs/mpnn/lib/python3.9/site-packages/haiku/_src/base.py:406: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return init(shape, dtype)\n",
      "/root/anaconda3/envs/mpnn/lib/python3.9/site-packages/haiku/_src/data_structures.py:143: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.\n",
      "  leaves, treedef = jax.tree_flatten(tree)\n",
      "/root/anaconda3/envs/mpnn/lib/python3.9/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.\n",
      "  return jax.tree_unflatten(treedef, leaves)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-18 12:04:42.326668 Iteration    0 | Batch loss 4.190897 Batch accuracy 0.54\n",
      "2023-03-18 12:04:42.393625 Iteration  128 | Batch loss 0.766553 Batch accuracy 0.41\n",
      "2023-03-18 12:04:42.433375 Iteration  256 | Batch loss 1.418116 Batch accuracy 0.46\n",
      "2023-03-18 12:04:42.472408 Iteration  384 | Batch loss 0.680156 Batch accuracy 0.49\n",
      "2023-03-18 12:04:42.511329 Iteration  512 | Batch loss 0.535574 Batch accuracy 0.55\n",
      "2023-03-18 12:04:42.550231 Iteration  640 | Batch loss 0.490587 Batch accuracy 0.53\n",
      "2023-03-18 12:04:42.590626 Iteration  768 | Batch loss 0.352529 Batch accuracy 0.53\n",
      "2023-03-18 12:04:42.645173 Iteration  896 | Batch loss 0.448731 Batch accuracy 0.51\n",
      "2023-03-18 12:04:42.684754 Iteration 1024 | Batch loss 0.369457 Batch accuracy 0.55\n",
      "2023-03-18 12:04:42.724314 Iteration 1152 | Batch loss 0.472284 Batch accuracy 0.50\n",
      "2023-03-18 12:04:44.110854 Iteration 1280 | Batch loss 0.428029 Batch accuracy 0.59\n",
      "2023-03-18 12:04:44.111098 Epoch  1 completed!\n",
      "2023-03-18 12:04:44.111129 Epoch  1       | Train accuracy: 0.592 | Test accuracy: 0.535 | Combined accuracy: 0.581\n",
      "2023-03-18 12:04:45.299024 Iteration    0 | Batch loss 0.403073 Batch accuracy 0.55\n",
      "2023-03-18 12:04:45.339084 Iteration  128 | Batch loss 0.379758 Batch accuracy 0.59\n",
      "2023-03-18 12:04:45.378547 Iteration  256 | Batch loss 0.339074 Batch accuracy 0.59\n",
      "2023-03-18 12:04:45.418526 Iteration  384 | Batch loss 0.383739 Batch accuracy 0.54\n",
      "2023-03-18 12:04:45.457762 Iteration  512 | Batch loss 0.383257 Batch accuracy 0.61\n",
      "2023-03-18 12:04:45.527118 Iteration  640 | Batch loss 0.312459 Batch accuracy 0.70\n",
      "2023-03-18 12:04:45.594224 Iteration  768 | Batch loss 0.461066 Batch accuracy 0.57\n",
      "2023-03-18 12:04:45.650986 Iteration  896 | Batch loss 0.445169 Batch accuracy 0.57\n",
      "2023-03-18 12:04:45.691084 Iteration 1024 | Batch loss 0.346432 Batch accuracy 0.53\n",
      "2023-03-18 12:04:45.729895 Iteration 1152 | Batch loss 0.364372 Batch accuracy 0.55\n",
      "2023-03-18 12:04:45.755057 Iteration 1280 | Batch loss 0.462173 Batch accuracy 0.56\n",
      "2023-03-18 12:04:45.755131 Epoch  2 completed!\n",
      "2023-03-18 12:04:45.755141 Epoch  2       | Train accuracy: 0.550 | Test accuracy: 0.538 | Combined accuracy: 0.547\n",
      "2023-03-18 12:04:45.862397 Iteration    0 | Batch loss 0.339602 Batch accuracy 0.55\n",
      "2023-03-18 12:04:45.902166 Iteration  128 | Batch loss 0.374216 Batch accuracy 0.57\n",
      "2023-03-18 12:04:45.942564 Iteration  256 | Batch loss 0.438747 Batch accuracy 0.58\n",
      "2023-03-18 12:04:45.982546 Iteration  384 | Batch loss 0.353809 Batch accuracy 0.57\n",
      "2023-03-18 12:04:46.022535 Iteration  512 | Batch loss 0.349946 Batch accuracy 0.52\n",
      "2023-03-18 12:04:46.061689 Iteration  640 | Batch loss 0.353156 Batch accuracy 0.57\n",
      "2023-03-18 12:04:46.100924 Iteration  768 | Batch loss 0.346630 Batch accuracy 0.58\n",
      "2023-03-18 12:04:46.140591 Iteration  896 | Batch loss 0.318004 Batch accuracy 0.57\n",
      "2023-03-18 12:04:46.179919 Iteration 1024 | Batch loss 0.312657 Batch accuracy 0.66\n",
      "2023-03-18 12:04:46.218909 Iteration 1152 | Batch loss 0.368878 Batch accuracy 0.59\n",
      "2023-03-18 12:04:46.261178 Iteration 1280 | Batch loss 0.429334 Batch accuracy 0.66\n",
      "2023-03-18 12:04:46.261245 Epoch  3 completed!\n",
      "2023-03-18 12:04:46.261255 Epoch  3       | Train accuracy: 0.589 | Test accuracy: 0.511 | Combined accuracy: 0.573\n",
      "2023-03-18 12:04:46.407183 Iteration    0 | Batch loss 0.343451 Batch accuracy 0.54\n",
      "2023-03-18 12:04:46.461840 Iteration  128 | Batch loss 0.352880 Batch accuracy 0.54\n",
      "2023-03-18 12:04:46.517765 Iteration  256 | Batch loss 0.326834 Batch accuracy 0.61\n",
      "2023-03-18 12:04:46.573242 Iteration  384 | Batch loss 0.319024 Batch accuracy 0.55\n",
      "2023-03-18 12:04:46.614563 Iteration  512 | Batch loss 0.313064 Batch accuracy 0.65\n",
      "2023-03-18 12:04:46.672820 Iteration  640 | Batch loss 0.323168 Batch accuracy 0.60\n",
      "2023-03-18 12:04:46.726845 Iteration  768 | Batch loss 0.305615 Batch accuracy 0.61\n",
      "2023-03-18 12:04:46.766145 Iteration  896 | Batch loss 0.323711 Batch accuracy 0.56\n",
      "2023-03-18 12:04:46.805769 Iteration 1024 | Batch loss 0.327618 Batch accuracy 0.55\n",
      "2023-03-18 12:04:46.844512 Iteration 1152 | Batch loss 0.356704 Batch accuracy 0.55\n",
      "2023-03-18 12:04:46.870200 Iteration 1280 | Batch loss 0.511086 Batch accuracy 0.50\n",
      "2023-03-18 12:04:46.870265 Epoch  4 completed!\n",
      "2023-03-18 12:04:46.870275 Epoch  4       | Train accuracy: 0.591 | Test accuracy: 0.538 | Combined accuracy: 0.580\n",
      "2023-03-18 12:04:46.995051 Iteration    0 | Batch loss 0.339471 Batch accuracy 0.58\n",
      "2023-03-18 12:04:47.035122 Iteration  128 | Batch loss 0.337709 Batch accuracy 0.60\n",
      "2023-03-18 12:04:47.074352 Iteration  256 | Batch loss 0.339430 Batch accuracy 0.62\n",
      "2023-03-18 12:04:47.113788 Iteration  384 | Batch loss 0.344836 Batch accuracy 0.50\n",
      "2023-03-18 12:04:47.153234 Iteration  512 | Batch loss 0.346657 Batch accuracy 0.55\n",
      "2023-03-18 12:04:47.193190 Iteration  640 | Batch loss 0.341154 Batch accuracy 0.66\n",
      "2023-03-18 12:04:47.232971 Iteration  768 | Batch loss 0.322930 Batch accuracy 0.59\n",
      "2023-03-18 12:04:47.272125 Iteration  896 | Batch loss 0.345116 Batch accuracy 0.59\n",
      "2023-03-18 12:04:47.311658 Iteration 1024 | Batch loss 0.346141 Batch accuracy 0.60\n",
      "2023-03-18 12:04:47.351395 Iteration 1152 | Batch loss 0.333005 Batch accuracy 0.50\n",
      "2023-03-18 12:04:47.376448 Iteration 1280 | Batch loss 0.329649 Batch accuracy 0.47\n",
      "2023-03-18 12:04:47.376515 Epoch  5 completed!\n",
      "2023-03-18 12:04:47.376525 Epoch  5       | Train accuracy: 0.533 | Test accuracy: 0.523 | Combined accuracy: 0.531\n",
      "2023-03-18 12:04:47.485901 Iteration    0 | Batch loss 0.323737 Batch accuracy 0.55\n",
      "2023-03-18 12:04:47.525502 Iteration  128 | Batch loss 0.325805 Batch accuracy 0.66\n",
      "2023-03-18 12:04:47.564863 Iteration  256 | Batch loss 0.314357 Batch accuracy 0.55\n",
      "2023-03-18 12:04:47.603824 Iteration  384 | Batch loss 0.324739 Batch accuracy 0.60\n",
      "2023-03-18 12:04:47.643720 Iteration  512 | Batch loss 0.320550 Batch accuracy 0.60\n",
      "2023-03-18 12:04:47.700140 Iteration  640 | Batch loss 0.336711 Batch accuracy 0.53\n",
      "2023-03-18 12:04:47.742555 Iteration  768 | Batch loss 0.322804 Batch accuracy 0.57\n",
      "2023-03-18 12:04:47.782209 Iteration  896 | Batch loss 0.331341 Batch accuracy 0.54\n",
      "2023-03-18 12:04:47.821725 Iteration 1024 | Batch loss 0.329188 Batch accuracy 0.65\n",
      "2023-03-18 12:04:47.860939 Iteration 1152 | Batch loss 0.318657 Batch accuracy 0.59\n",
      "2023-03-18 12:04:47.885402 Iteration 1280 | Batch loss 0.315856 Batch accuracy 0.62\n",
      "2023-03-18 12:04:47.885469 Epoch  6 completed!\n",
      "2023-03-18 12:04:47.885479 Epoch  6       | Train accuracy: 0.588 | Test accuracy: 0.544 | Combined accuracy: 0.580\n",
      "2023-03-18 12:04:47.980465 Iteration    0 | Batch loss 0.355247 Batch accuracy 0.54\n",
      "2023-03-18 12:04:48.020256 Iteration  128 | Batch loss 0.312928 Batch accuracy 0.58\n",
      "2023-03-18 12:04:48.060040 Iteration  256 | Batch loss 0.330980 Batch accuracy 0.58\n",
      "2023-03-18 12:04:48.113291 Iteration  384 | Batch loss 0.335711 Batch accuracy 0.55\n",
      "2023-03-18 12:04:48.153201 Iteration  512 | Batch loss 0.332973 Batch accuracy 0.56\n",
      "2023-03-18 12:04:48.194381 Iteration  640 | Batch loss 0.318438 Batch accuracy 0.59\n",
      "2023-03-18 12:04:48.234094 Iteration  768 | Batch loss 0.311218 Batch accuracy 0.60\n",
      "2023-03-18 12:04:48.273371 Iteration  896 | Batch loss 0.323734 Batch accuracy 0.62\n",
      "2023-03-18 12:04:48.312144 Iteration 1024 | Batch loss 0.317123 Batch accuracy 0.62\n",
      "2023-03-18 12:04:48.351828 Iteration 1152 | Batch loss 0.331426 Batch accuracy 0.58\n",
      "2023-03-18 12:04:48.377779 Iteration 1280 | Batch loss 0.294581 Batch accuracy 0.72\n",
      "2023-03-18 12:04:48.377844 Epoch  7 completed!\n",
      "2023-03-18 12:04:48.377854 Epoch  7       | Train accuracy: 0.592 | Test accuracy: 0.541 | Combined accuracy: 0.582\n",
      "2023-03-18 12:04:48.502097 Iteration    0 | Batch loss 0.320362 Batch accuracy 0.60\n",
      "2023-03-18 12:04:48.541087 Iteration  128 | Batch loss 0.318596 Batch accuracy 0.54\n",
      "2023-03-18 12:04:48.593999 Iteration  256 | Batch loss 0.324454 Batch accuracy 0.54\n",
      "2023-03-18 12:04:48.634091 Iteration  384 | Batch loss 0.306254 Batch accuracy 0.66\n",
      "2023-03-18 12:04:48.673850 Iteration  512 | Batch loss 0.323603 Batch accuracy 0.66\n",
      "2023-03-18 12:04:48.713990 Iteration  640 | Batch loss 0.307959 Batch accuracy 0.63\n",
      "2023-03-18 12:04:48.753627 Iteration  768 | Batch loss 0.348375 Batch accuracy 0.55\n",
      "2023-03-18 12:04:48.793319 Iteration  896 | Batch loss 0.343689 Batch accuracy 0.60\n",
      "2023-03-18 12:04:48.834150 Iteration 1024 | Batch loss 0.309230 Batch accuracy 0.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-18 12:04:48.887944 Iteration 1152 | Batch loss 0.330533 Batch accuracy 0.59\n",
      "2023-03-18 12:04:48.912789 Iteration 1280 | Batch loss 0.274451 Batch accuracy 0.72\n",
      "2023-03-18 12:04:48.912849 Epoch  8 completed!\n",
      "2023-03-18 12:04:48.912858 Epoch  8       | Train accuracy: 0.532 | Test accuracy: 0.526 | Combined accuracy: 0.531\n",
      "2023-03-18 12:04:49.004963 Iteration    0 | Batch loss 0.373263 Batch accuracy 0.48\n",
      "2023-03-18 12:04:49.044091 Iteration  128 | Batch loss 0.337002 Batch accuracy 0.59\n",
      "2023-03-18 12:04:49.083164 Iteration  256 | Batch loss 0.313133 Batch accuracy 0.53\n",
      "2023-03-18 12:04:49.122248 Iteration  384 | Batch loss 0.335329 Batch accuracy 0.59\n",
      "2023-03-18 12:04:49.161865 Iteration  512 | Batch loss 0.318038 Batch accuracy 0.62\n",
      "2023-03-18 12:04:49.201358 Iteration  640 | Batch loss 0.325529 Batch accuracy 0.60\n",
      "2023-03-18 12:04:49.239720 Iteration  768 | Batch loss 0.308404 Batch accuracy 0.62\n",
      "2023-03-18 12:04:49.279591 Iteration  896 | Batch loss 0.343851 Batch accuracy 0.48\n",
      "2023-03-18 12:04:49.334491 Iteration 1024 | Batch loss 0.295809 Batch accuracy 0.62\n",
      "2023-03-18 12:04:49.374983 Iteration 1152 | Batch loss 0.314699 Batch accuracy 0.55\n",
      "2023-03-18 12:04:49.401054 Iteration 1280 | Batch loss 0.400712 Batch accuracy 0.44\n",
      "2023-03-18 12:04:49.401124 Epoch  9 completed!\n",
      "2023-03-18 12:04:49.401134 Epoch  9       | Train accuracy: 0.545 | Test accuracy: 0.529 | Combined accuracy: 0.542\n",
      "2023-03-18 12:04:49.496627 Iteration    0 | Batch loss 0.318720 Batch accuracy 0.59\n",
      "2023-03-18 12:04:49.535795 Iteration  128 | Batch loss 0.295661 Batch accuracy 0.68\n",
      "2023-03-18 12:04:49.576047 Iteration  256 | Batch loss 0.341906 Batch accuracy 0.59\n",
      "2023-03-18 12:04:49.616251 Iteration  384 | Batch loss 0.348019 Batch accuracy 0.60\n",
      "2023-03-18 12:04:49.670506 Iteration  512 | Batch loss 0.341869 Batch accuracy 0.56\n",
      "2023-03-18 12:04:49.710087 Iteration  640 | Batch loss 0.301123 Batch accuracy 0.57\n",
      "2023-03-18 12:04:49.749365 Iteration  768 | Batch loss 0.342839 Batch accuracy 0.54\n",
      "2023-03-18 12:04:49.788900 Iteration  896 | Batch loss 0.373229 Batch accuracy 0.54\n",
      "2023-03-18 12:04:49.827920 Iteration 1024 | Batch loss 0.305334 Batch accuracy 0.60\n",
      "2023-03-18 12:04:49.892617 Iteration 1152 | Batch loss 0.317465 Batch accuracy 0.57\n",
      "2023-03-18 12:04:49.918763 Iteration 1280 | Batch loss 0.321877 Batch accuracy 0.50\n",
      "2023-03-18 12:04:49.918829 Epoch 10 completed!\n",
      "2023-03-18 12:04:49.918838 Epoch 10       | Train accuracy: 0.591 | Test accuracy: 0.541 | Combined accuracy: 0.581\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "trained_params = model.net.init(\n",
    "    jax.random.PRNGKey(42),\n",
    "    features=train_dataset.features[0],\n",
    "    rows=train_dataset.rows[0],\n",
    "    cols=train_dataset.columns[0],\n",
    "    batch_size=1,\n",
    "    masks=train_dataset.features[0][np.newaxis, :, :])\n",
    "trained_opt_state = opt_init(trained_params)\n",
    "\n",
    "for ep in range(1, num_epochs + 1):\n",
    "    tr_data = list(\n",
    "        zip(\n",
    "            train_dataset.features,\n",
    "            train_dataset.rows,\n",
    "            train_dataset.columns,\n",
    "            train_dataset.labels,\n",
    "            train_dataset.edge_types,\n",
    "        ))\n",
    "    random.shuffle(tr_data)\n",
    "    features_train, rows_train, cols_train, ys_train, edge_types_train = zip(\n",
    "        *tr_data)\n",
    "\n",
    "    features_train = list(features_train)\n",
    "    rows_train = list(rows_train)\n",
    "    cols_train = list(cols_train)\n",
    "    ys_train = np.array(ys_train)\n",
    "    edge_types_train = list(edge_types_train)\n",
    "\n",
    "    for i in range(0, len(features_train), batch_size):\n",
    "        b_features, b_rows, b_cols, b_ys, b_edges = batch(\n",
    "            features_train[i:i + batch_size],\n",
    "            rows_train[i:i + batch_size],\n",
    "            cols_train[i:i + batch_size],\n",
    "            ys_train[i:i + batch_size],\n",
    "            edge_types_train[i:i + batch_size],\n",
    "        )\n",
    "\n",
    "        trained_params, trained_opt_state, curr_loss = train(\n",
    "            loss_val_gr,\n",
    "            opt_update,\n",
    "            trained_params,\n",
    "            trained_opt_state,\n",
    "            b_features,\n",
    "            b_rows,\n",
    "            b_cols,\n",
    "            b_ys,\n",
    "            b_edges,\n",
    "        )\n",
    "\n",
    "        accs = model.accuracy(\n",
    "            trained_params,\n",
    "            b_features,\n",
    "            b_rows,\n",
    "            b_cols,\n",
    "            b_ys,\n",
    "            b_edges,\n",
    "        )\n",
    "        print(datetime.datetime.now(),\n",
    "              f\"Iteration {i:4d} | Batch loss {curr_loss:.6f}\",\n",
    "              f\"Batch accuracy {accs:.2f}\")\n",
    "\n",
    "    print(datetime.datetime.now(), f\"Epoch {ep:2d} completed!\")\n",
    "\n",
    "    # Calculate accuracy across full dataset once per epoch\n",
    "    print(datetime.datetime.now(), f\"Epoch {ep:2d}       | \", end=\"\")\n",
    "    print_accuracies(model,trained_params, test_dataset, train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpnn",
   "language": "python",
   "name": "mpnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
