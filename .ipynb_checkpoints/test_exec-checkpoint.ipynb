{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84dd03b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-18 12:02:18.090871: W external/org_tensorflow/tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64::/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/extras/CUPTI/lib64\n",
      "2023-03-18 12:02:18.092418: W external/org_tensorflow/tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64::/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/extras/CUPTI/lib64\n",
      "/root/anaconda3/envs/mpnn/lib/python3.9/site-packages/haiku/_src/data_structures.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import dataclasses\n",
    "import datetime\n",
    "import enum\n",
    "import functools\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import tempfile\n",
    "from typing import Sequence\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import optax\n",
    "import psutil\n",
    "import scipy.sparse as sp\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92abc7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'batch' from 'Load_Data' (/root/Min/Ptab/Load_Data.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mModel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model,Direction,Reduction\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# from Load_Data import batch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train,print_accuracies\n",
      "File \u001b[0;32m~/Min/Ptab/Train.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptax\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mLoad_Data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m batch\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(loss_val_gr,opt_update,params, opt_state, features, rows, cols, ys, masks):\n\u001b[1;32m      5\u001b[0m   curr_loss, gradient \u001b[38;5;241m=\u001b[39m loss_val_gr(params, features, rows, cols, ys, masks)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'batch' from 'Load_Data' (/root/Min/Ptab/Load_Data.py)"
     ]
    }
   ],
   "source": [
    "from BH.data_loader import *\n",
    "from BH.generate_data import *\n",
    "from Model import Model,Direction,Reduction\n",
    "# from Load_Data import batch\n",
    "from Train import train,print_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea689be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load data\n",
    "\n",
    "#@markdown Training this model is pretty slow - an hour or so on the free tier colab, but subject to inactivity timeouts and pre-emptions.\n",
    "\n",
    "#@markdown In order to make it possible to recreate the results from the paper reliably and quickly, we provide several helpers to either speed things up, or reduce the memory footprint:\n",
    "#@markdown * Pretrained weights - greatly speeds things up by loading the trained model parameters rather than learning from the data\n",
    "#@markdown * If you are running on a high memory machine (ie *not* on the free colab instance!) the input graph data can be loaded from a pickle (which is faster to load) and kept in memory (faster to re-use, but uses ~12Gb of memory). This makes no difference to training speed (it's only relevant for `generate_graph_data()` and `get_saliency_vectors()`).\n",
    "DIR_PATH = \"/Data/Ptab/n=5\"\n",
    "\n",
    "\n",
    "use_pretrained_weights = True  #@param{type:\"boolean\"}\n",
    "hold_graphs_in_memory = False  #@param{type:\"boolean\"}\n",
    "\n",
    "gb = 1024**3\n",
    "total_memory = psutil.virtual_memory().total / gb\n",
    "# Less than 20Gb of RAM means we need to do some things slower, but with lower memory impact - in\n",
    "# particular, we want to allow things to run on the free colab tier.\n",
    "if total_memory < 20 and hold_graphs_in_memory:\n",
    "    raise RuntimeError(f\"It is unlikely your machine (with {total_memory}Gb) will have enough memory to complete the colab's execution!\")\n",
    "\n",
    "print(\"Loading input data...\")\n",
    "full_dataset, train_dataset, test_dataset = load_input_data(DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3464d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Network Setup\n",
    "\n",
    "step_size = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "num_classes = np.max(train_dataset.labels) + 1\n",
    "model = Model(\n",
    "    num_layers=3,\n",
    "    num_features=64,\n",
    "    num_classes=num_classes,\n",
    "    direction=Direction.BOTH,\n",
    "    reduction=Reduction.SUM,\n",
    "    apply_relu_activation=True,\n",
    "    use_mask=False,\n",
    "    share=False,\n",
    "    message_relu=True,\n",
    "    with_bias=True)\n",
    "\n",
    "loss_val_gr = jax.value_and_grad(model.loss)\n",
    "opt_init, opt_update = optax.adam(step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2cbb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3516236",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "trained_params = model.net.init(\n",
    "    jax.random.PRNGKey(42),\n",
    "    features=train_dataset.features[0],\n",
    "    rows=train_dataset.rows[0],\n",
    "    cols=train_dataset.columns[0],\n",
    "    batch_size=1,\n",
    "    masks=train_dataset.features[0][np.newaxis, :, :])\n",
    "trained_opt_state = opt_init(trained_params)\n",
    "\n",
    "for ep in range(1, num_epochs + 1):\n",
    "    tr_data = list(\n",
    "        zip(\n",
    "            train_dataset.features,\n",
    "            train_dataset.rows,\n",
    "            train_dataset.columns,\n",
    "            train_dataset.labels,\n",
    "            train_dataset.edge_types,\n",
    "        ))\n",
    "    random.shuffle(tr_data)\n",
    "    features_train, rows_train, cols_train, ys_train, edge_types_train = zip(\n",
    "        *tr_data)\n",
    "\n",
    "    features_train = list(features_train)\n",
    "    rows_train = list(rows_train)\n",
    "    cols_train = list(cols_train)\n",
    "    ys_train = np.array(ys_train)\n",
    "    edge_types_train = list(edge_types_train)\n",
    "\n",
    "    for i in range(0, len(features_train), batch_size):\n",
    "        b_features, b_rows, b_cols, b_ys, b_edges = batch(\n",
    "            features_train[i:i + batch_size],\n",
    "            rows_train[i:i + batch_size],\n",
    "            cols_train[i:i + batch_size],\n",
    "            ys_train[i:i + batch_size],\n",
    "            edge_types_train[i:i + batch_size],\n",
    "        )\n",
    "\n",
    "        trained_params, trained_opt_state, curr_loss = train(\n",
    "            loss_val_gr,\n",
    "            opt_update,\n",
    "            trained_params,\n",
    "            trained_opt_state,\n",
    "            b_features,\n",
    "            b_rows,\n",
    "            b_cols,\n",
    "            b_ys,\n",
    "            b_edges,\n",
    "        )\n",
    "\n",
    "        accs = model.accuracy(\n",
    "            trained_params,\n",
    "            b_features,\n",
    "            b_rows,\n",
    "            b_cols,\n",
    "            b_ys,\n",
    "            b_edges,\n",
    "        )\n",
    "        print(datetime.datetime.now(),\n",
    "              f\"Iteration {i:4d} | Batch loss {curr_loss:.6f}\",\n",
    "              f\"Batch accuracy {accs:.2f}\")\n",
    "\n",
    "    print(datetime.datetime.now(), f\"Epoch {ep:2d} completed!\")\n",
    "\n",
    "    # Calculate accuracy across full dataset once per epoch\n",
    "    print(datetime.datetime.now(), f\"Epoch {ep:2d}       | \", end=\"\")\n",
    "    print_accuracies(model,trained_params, test_dataset, train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpnn",
   "language": "python",
   "name": "mpnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
